---
title: "Estimating Equations for Logistic Regression"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
```


\newcommand{\tp}{\mathsf{T}}


# Parametric Regression Models

Let $Y$ be a response of interest and $X = (X_1, \ldots, X_p)^\tp$ a vector explanatory variables.  In Section 2.2, Brumback considers three parametric models for the conditional expectation $E(Y| X)$. 

- The linear model: 
\begin{align*}
E(Y| X; \beta) = X^\tp \beta.
\end{align*}
- The log-linear model: 
\begin{align*}
\operatorname{log}(E(Y| X; \beta)) & = X^\tp \beta, \\
E(Y| X; \beta) & = \operatorname{exp}(X^\tp \beta; \beta).\\
\end{align*}
- The logistic model: 
\begin{align*}
\operatorname{logit}(E(Y| X; \beta)) & = X^\tp \beta, \\
E(Y| X; \beta) & = \operatorname{expit}(X^\tp \beta), \\ 
\end{align*}
where $\operatorname{logit}(p) = \operatorname{log} \left( \frac{p}{1 - p} \right)$ and $\operatorname{exp}(x) =  \frac{\operatorname{exp}(x)}{1 - \operatorname{exp}(x)}$.


# Estimating Equations

In Section 2.3, Brumback considers how to estimate these models from a sample $(X_i, Y_i), i = 1, \ldots, n$.  In particular she proposes to use the following estimating equation:
\begin{align*}
U(\beta) = \sum_i X_i^\mathsf{T} (Y_i - E(Y_i| X_i; \beta)) = 0.
\end{align*}
Where does this estimating equation come from?  Why should we use it?  For the linear model, solving $U(\beta) = 0$ is equivalent to minimizing the sum of squares, i.e. $U(\beta) = \frac{\partial Q}{\partial \beta}$ where $Q(\beta) = \sum_i (Y_i - X_i^\tp \beta)^2$.  On the other hand, $U(\beta) \neq \frac{\partial Q}{\partial \beta}$ for the other two models.

Brumback says the estimating equation can be solved iteratively using Newton-Raphson.  For the logistic model in particular, she says that in `R` we can use `glm` with `family = binomial`.  Since the GLM estimator uses the method of maximum likelihood, this suggests that the estimating equations may be equivalent to maximum likelihood.  However, I didn't immediately recognize their equivalence.  Below I consider a logistic regression example and compare the solution to the normal equations with the GLM estimator.


# Example Comparing EE and GLM Estimators

```{r data, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)

# Reconstruct the data from the What-If study in Table 1.3
whatif_compact <- expand_grid(T = c(0, 1), A = c(0, 1), H = c(0, 1), Y = c(0, 1)) %>% 
  mutate(n = c(15, 3, 3, 11, 36, 4, 4, 9, 15, 3, 3, 7, 27, 3, 9, 13))
whatif <- whatif_compact %>% 
  uncount(n)

# GLM estimator for the logistic model
fit_glm <- glm(formula = Y ~ A + T + H, family = binomial, data = whatif, weights = whatif$n)

# Solve the estimating equations with Newton-Raphson
X <- model.matrix(fit_glm)
Y <- whatif$Y

# Link function and its derivative
g <- function(x) exp(x)/(1 + exp(x))
g_prime <- function(x) exp(x)/(1 + exp(x))^2
# Objective function U(beta)
U <- function(beta) rowSums(t(X) %*% (Y - g(X %*% beta)))
# Jacobian of objective function
J <- function(beta) t(X) %*% diag(-drop(g_prime(X %*% beta))) %*% X
# Newton-Raphson update
NR <- function(x_n, J_n, U_n) {
  x_diff <- solve(J(x_n), -U_n)
  x_new <- x_n + x_diff
  return(x_new)
}

# Do 10 Newton-Raphson iterations
beta_hat <- c(0, 0, 0, 0)
for (i in 1:10) beta_hat <- NR(beta_hat, J(beta_hat), U(beta_hat))

# Compare the estimates
coef(fit_glm)
beta_hat
coef(fit_glm) - beta_hat
```
We see that the solution to the estimating equation coincides with the GLM estimator.







